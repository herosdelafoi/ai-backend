# Guide de Test Postman - Day 5 AI Backend

## Installation et DÃ©marrage

### 1. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 2. Configurer votre clÃ© API OpenAI
Ouvrez le fichier `.env` et remplacez `your-openai-api-key-here` par votre vraie clÃ© API OpenAI :
```env
OPENAI_API_KEY=sk-...votre-clÃ©-ici...
```

### 3. Lancer le serveur
```bash
uvicorn app.main:app --reload --port 8000
```

Vous devriez voir :
```
ğŸš€ Starting AI Backend...
INFO:     Uvicorn running on http://127.0.0.1:8000
```

### 4. Tester la santÃ© de l'API
Ouvrez votre navigateur : http://localhost:8000/health

Vous devriez voir : `{"status": "healthy", "version": "1.0.0"}`

---

## Tests avec Postman

### ğŸ“– Documentation Interactive
Visitez http://localhost:8000/docs pour voir la documentation Swagger interactive gÃ©nÃ©rÃ©e automatiquement par FastAPI.

---

## Endpoints Ã  Tester

### 1ï¸âƒ£ Health Check
**GET** `http://localhost:8000/health`

âœ… **RÃ©ponse attendue:**
```json
{
  "status": "healthy",
  "version": "1.0.0"
}
```

---

### 2ï¸âƒ£ Chat Simple (Sans historique)
**POST** `http://localhost:8000/api/chat/`

**Headers:**
- `Content-Type: application/json`

**Body (raw JSON):**
```json
{
  "message": "Qu'est-ce que FastAPI ?",
  "temperature": 0.7
}
```

âœ… **RÃ©ponse attendue:**
```json
{
  "response": "FastAPI est un framework web moderne...",
  "conversation_id": "uuid-gÃ©nÃ©rÃ©-automatiquement",
  "tokens_used": 150,
  "model": "gpt-4-turbo",
  "created_at": "2026-01-23T..."
}
```

**ğŸ¯ Concept clÃ©:** Gestion des requÃªtes/rÃ©ponses avec Pydantic models

---

### 3ï¸âƒ£ Chat avec Historique de Conversation
**POST** `http://localhost:8000/api/chat/`

**Body:**
```json
{
  "message": "Donne-moi plus de dÃ©tails",
  "conversation_id": "COLLER-ICI-LE-conversation_id-DU-MESSAGE-PRECEDENT",
  "temperature": 0.7
}
```

âœ… **RÃ©sultat:** L'IA se souviendra du contexte de la conversation prÃ©cÃ©dente.

**ğŸ¯ Concept clÃ©:** Gestion d'Ã©tat des conversations en mÃ©moire

---

### 4ï¸âƒ£ Chat avec System Prompt PersonnalisÃ©
**POST** `http://localhost:8000/api/chat/`

**Body:**
```json
{
  "message": "Bonjour",
  "system_prompt": "Tu es un pirate qui parle comme Jack Sparrow",
  "temperature": 0.9
}
```

âœ… **RÃ©sultat:** L'IA rÃ©pondra en mode pirate !

**ğŸ¯ Concept clÃ©:** Personnalisation du comportement via system prompts

---

### 5ï¸âƒ£ Chat en Streaming (SSE)
**POST** `http://localhost:8000/api/chat/stream`

**Body:**
```json
{
  "message": "Ã‰cris un court poÃ¨me sur le code",
  "temperature": 0.9
}
```

âœ… **RÃ©ponse attendue:** Flux de donnÃ©es en temps rÃ©el au format SSE
```
data: {"content": "Dans"}

data: {"content": " les"}

data: {"content": " lignes"}

...

data: [DONE]
```

**ğŸ¯ Concept clÃ©:** Streaming responses avec Server-Sent Events

---

### 6ï¸âƒ£ Analyse de Document
**POST** `http://localhost:8000/api/analysis/document`

**Headers:**
- `Content-Type: application/x-www-form-urlencoded`

**Body (x-www-form-urlencoded):**
- Key: `text`
- Value: `This is an amazing product! I absolutely love the quality and the customer service was outstanding. Highly recommended to everyone!`

âœ… **RÃ©ponse attendue:**
```json
{
  "summary": "Avis trÃ¨s positif sur un produit...",
  "sentiment": {
    "sentiment": "POSITIVE",
    "confidence": 0.95,
    "explanation": "Utilisation de mots trÃ¨s positifs..."
  },
  "entities": [
    {
      "text": "customer service",
      "type": "ORG",
      "start": 65,
      "end": 81
    }
  ],
  "key_points": [
    "Produit de qualitÃ©",
    "Excellent service client",
    "Fortement recommandÃ©"
  ],
  "tokens_used": 200
}
```

**ğŸ¯ Concept clÃ©:** Structured outputs - Parser des rÃ©ponses JSON du LLM

---

### 7ï¸âƒ£ Classification de Texte
**POST** `http://localhost:8000/api/analysis/classify`

**Headers:**
- `Content-Type: application/json`

**Body (raw JSON):**
```json
{
  "text": "Je n'arrive pas Ã  rÃ©initialiser mon mot de passe",
  "categories": ["Support Technique", "Facturation", "Question GÃ©nÃ©rale", "Demande de FonctionnalitÃ©"]
}
```

âœ… **RÃ©ponse attendue:**
```json
{
  "category": "Support Technique",
  "confidence": 0.92,
  "reasoning": "La question concerne un problÃ¨me technique avec la rÃ©initialisation du mot de passe"
}
```

**ğŸ¯ Concept clÃ©:** Classification intelligente avec LLM

---

### 8ï¸âƒ£ Traitement par Batch (ParallÃ¨le)
**POST** `http://localhost:8000/api/analysis/batch`

**Body:**
```json
{
  "texts": [
    "Ce produit est incroyable !",
    "ExpÃ©rience terrible, trÃ¨s dÃ©Ã§u.",
    "C'est correct, rien de spÃ©cial.",
    "Le meilleur achat de ma vie !"
  ],
  "operation": "sentiment"
}
```

âœ… **RÃ©ponse attendue:**
```json
{
  "results": [
    {
      "text": "Ce produit est incroyable !",
      "result": "POSITIVE",
      "tokens": 25
    },
    {
      "text": "ExpÃ©rience terrible, trÃ¨s dÃ©Ã§u.",
      "result": "NEGATIVE",
      "tokens": 28
    },
    {
      "text": "C'est correct, rien de spÃ©cial.",
      "result": "NEUTRAL",
      "tokens": 27
    },
    {
      "text": "Le meilleur achat de ma vie !",
      "result": "POSITIVE",
      "tokens": 26
    }
  ],
  "total_tokens": 106
}
```

**Essayez aussi avec `"operation": "summarize"`**

**ğŸ¯ Concept clÃ©:** Traitement asynchrone parallÃ¨le avec asyncio.gather

---

### 9ï¸âƒ£ Test du Rate Limiting
**Objectif:** Envoyer 101 requÃªtes rapidement pour dÃ©clencher le rate limiter

Dans Postman, utilisez le **Collection Runner** :
1. CrÃ©ez une collection avec n'importe quel endpoint
2. Lancez le Runner avec 101 itÃ©rations
3. Observez les rÃ©ponses

âœ… **RÃ©sultat attendu aprÃ¨s 100 requÃªtes:**
```json
{
  "detail": "Rate limit exceeded. Please wait before making more requests."
}
```
**Status Code:** `429 Too Many Requests`

**ğŸ¯ Concept clÃ©:** Middleware de rate limiting pour protÃ©ger l'API

---

### ğŸ”Ÿ Test de l'Authentification (Optionnel)
L'authentification est implÃ©mentÃ©e mais pas activÃ©e par dÃ©faut. Pour tester:

**Headers:**
- `X-API-Key: demo-key-123`

**ClÃ©s valides:**
- `demo-key-123`
- `prod-key-456`

**ğŸ¯ Concept clÃ©:** API Key authentication avec FastAPI Security

---

## ğŸ“ Concepts ClÃ©s Ã  Retenir

### 1. **Architecture FastAPI**
- âœ… SÃ©paration claire: routers / services / models / middleware
- âœ… Dependency injection avec `Depends`
- âœ… Validation automatique avec Pydantic

### 2. **LLM Service Abstraction**
- âœ… Wrapper autour d'AsyncOpenAI
- âœ… Facilite le changement de provider
- âœ… Singleton pattern pour rÃ©utilisation

### 3. **Gestion d'Ã‰tat**
- âœ… ConversationService en mÃ©moire
- âœ… TTL automatique (60 minutes)
- âœ… Cleanup automatique toutes les 5 minutes

### 4. **Middleware Stack**
- âœ… CORS pour frontend
- âœ… Logging structurÃ© (JSON)
- âœ… Rate limiting par IP
- âœ… Auth avec API keys

### 5. **Streaming**
- âœ… Server-Sent Events (SSE)
- âœ… AsyncIterator pour rÃ©ponses en temps rÃ©el
- âœ… Format: `data: {json}\n\n`

### 6. **Structured Outputs**
- âœ… Prompts avec instructions JSON
- âœ… Parsing et validation
- âœ… Error handling robuste

### 7. **Traitement ParallÃ¨le**
- âœ… `asyncio.gather()` pour batch
- âœ… EfficacitÃ© avec concurrent processing
- âœ… Gestion d'erreurs individuelles

---

## ğŸ› DÃ©pannage

### Erreur: "openai_api_key not found"
â¡ï¸ **Solution:** VÃ©rifiez que votre `.env` contient `OPENAI_API_KEY=sk-...`

### Erreur: "Module 'openai' not found"
â¡ï¸ **Solution:** `pip install -r requirements.txt`

### Le serveur ne dÃ©marre pas
â¡ï¸ **Solution:** VÃ©rifiez que le port 8000 n'est pas dÃ©jÃ  utilisÃ©

### Rate limiting trop strict
â¡ï¸ **Solution:** Modifiez `RATE_LIMIT_REQUESTS` dans `.env`

### Les conversations ne persistent pas
â¡ï¸ **C'est normal !** Le systÃ¨me utilise la mÃ©moire (in-memory storage). Les conversations sont perdues au redÃ©marrage. C'est intentionnel pour l'apprentissage.

---

## ğŸ“Š Architecture RÃ©sumÃ©e

```
Client (Postman)
    â†“
FastAPI (main.py)
    â†“
Middleware Stack
    â”œâ”€ CORS
    â”œâ”€ Logging
    â””â”€ Rate Limiting
    â†“
Routers
    â”œâ”€ /api/chat (chat.py)
    â””â”€ /api/analysis (analysis.py)
    â†“
Services
    â”œâ”€ LLMService (llm_service.py)
    â””â”€ ConversationService (conversation.py)
    â†“
OpenAI API
```

---

## âœ… Checklist de Validation

Avant de passer au Jour 6, assurez-vous de pouvoir :

- [ ] CrÃ©er une API FastAPI avec endpoints chat
- [ ] GÃ©rer l'Ã©tat des conversations
- [ ] ImplÃ©menter le streaming (SSE)
- [ ] Ajouter du rate limiting
- [ ] Structurer un projet backend IA proprement
- [ ] Parser les rÃ©ponses JSON des LLMs
- [ ] GÃ©rer les erreurs gracieusement

---

## ğŸš€ Prochaines Ã‰tapes

Maintenant que vous maÃ®trisez le backend, vous pouvez :
1. Ajouter une base de donnÃ©es PostgreSQL (remplacer in-memory)
2. ImplÃ©menter l'authentification JWT
3. Ajouter Redis pour le cache
4. CrÃ©er un frontend React
5. DÃ©ployer sur Railway/Render/Vercel

**Bon apprentissage ! ğŸ“**
